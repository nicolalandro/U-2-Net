nohup: ignoring input
---
train images:  187
---
---define optimizer...
---start training...
/usr/local/lib/python3.7/dist-packages/skimage/io/manage_plugins.py:23: UserWarning: Your installed pillow version is < 7.1.0. Several security issues (CVE-2020-11538, CVE-2020-10379, CVE-2020-10994, CVE-2020-10177) have been fixed in pillow 7.1.0 or higher. We recommend to upgrade this library.
  from .collection import imread_collection_wrapper
/home/supreme/.local/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.
  warnings.warn(warning.format(ret))
l0: 0.853018, l1: 0.850619, l2: 0.548027, l3: 0.674108, l4: 0.511948, l5: 0.752989, l6: 0.708966

[epoch:   1/100000, batch:     2/  187, ite: 1] train loss: 4.899674, tar: 0.853018 
l0: 1.330016, l1: 0.643667, l2: 0.403932, l3: 0.386138, l4: 0.244791, l5: 0.218583, l6: 0.202633

[epoch:   1/100000, batch:     4/  187, ite: 2] train loss: 4.164717, tar: 1.091517 
l0: 1.447219, l1: 0.420133, l2: 0.411836, l3: 0.516418, l4: 0.450571, l5: 0.231185, l6: 0.231316

[epoch:   1/100000, batch:     6/  187, ite: 3] train loss: 4.012704, tar: 1.210084 
l0: 1.269310, l1: 0.347392, l2: 0.383765, l3: 0.344192, l4: 0.420504, l5: 0.446155, l6: 0.454653

[epoch:   1/100000, batch:     8/  187, ite: 4] train loss: 3.926021, tar: 1.224890 
l0: 1.298982, l1: 0.218398, l2: 0.310971, l3: 0.147636, l4: 0.224800, l5: 0.129479, l6: 0.145801

[epoch:   1/100000, batch:    10/  187, ite: 5] train loss: 3.636030, tar: 1.239709 
l0: 0.927834, l1: 0.175418, l2: 0.359928, l3: 0.143886, l4: 0.335128, l5: 0.087834, l6: 0.107665

[epoch:   1/100000, batch:    12/  187, ite: 6] train loss: 3.386307, tar: 1.187730 
l0: 0.794478, l1: 0.222029, l2: 0.399232, l3: 0.211193, l4: 0.357515, l5: 0.342233, l6: 0.255450

[epoch:   1/100000, batch:    14/  187, ite: 7] train loss: 3.271425, tar: 1.131551 
l0: 0.687288, l1: 0.209772, l2: 0.386494, l3: 0.206460, l4: 0.385529, l5: 0.472865, l6: 0.318106

[epoch:   1/100000, batch:    16/  187, ite: 8] train loss: 3.195811, tar: 1.076018 
l0: 0.622276, l1: 0.302303, l2: 0.387773, l3: 0.326199, l4: 0.442390, l5: 0.876021, l6: 0.434821

[epoch:   1/100000, batch:    18/  187, ite: 9] train loss: 3.217586, tar: 1.025602 
l0: 0.626027, l1: 0.216363, l2: 0.334291, l3: 0.208777, l4: 0.399786, l5: 0.406959, l6: 0.300303

[epoch:   1/100000, batch:    20/  187, ite: 10] train loss: 3.145078, tar: 0.985645 
l0: 0.572329, l1: 0.154666, l2: 0.323917, l3: 0.288886, l4: 0.332556, l5: 0.990108, l6: 0.276894

[epoch:   1/100000, batch:    22/  187, ite: 11] train loss: 3.126376, tar: 0.948071 
l0: 0.516305, l1: 0.257463, l2: 0.320085, l3: 0.242239, l4: 0.393877, l5: 0.317221, l6: 0.237053

[epoch:   1/100000, batch:    24/  187, ite: 12] train loss: 3.056198, tar: 0.912090 
l0: 0.548790, l1: 0.179791, l2: 0.279758, l3: 0.186974, l4: 0.268526, l5: 0.146642, l6: 0.142572

[epoch:   1/100000, batch:    26/  187, ite: 13] train loss: 2.955956, tar: 0.884144 
l0: 0.546928, l1: 0.126027, l2: 0.227363, l3: 0.143226, l4: 0.234939, l5: 0.122106, l6: 0.185368

[epoch:   1/100000, batch:    28/  187, ite: 14] train loss: 2.858099, tar: 0.860057 
l0: 0.645849, l1: 0.213425, l2: 0.234230, l3: 0.206863, l4: 0.228262, l5: 0.729330, l6: 0.213653

[epoch:   1/100000, batch:    30/  187, ite: 15] train loss: 2.832333, tar: 0.845777 
l0: 0.525014, l1: 0.131049, l2: 0.205151, l3: 0.134243, l4: 0.188722, l5: 0.116063, l6: 0.130105

[epoch:   1/100000, batch:    32/  187, ite: 16] train loss: 2.744709, tar: 0.825729 
l0: 0.525958, l1: 0.085900, l2: 0.187952, l3: 0.102037, l4: 0.128907, l5: 0.122543, l6: 0.083836

[epoch:   1/100000, batch:    34/  187, ite: 17] train loss: 2.656029, tar: 0.808095 
l0: 0.510228, l1: 0.127125, l2: 0.196347, l3: 0.109434, l4: 0.143531, l5: 0.110330, l6: 0.108104

[epoch:   1/100000, batch:    36/  187, ite: 18] train loss: 2.580977, tar: 0.791547 
l0: 0.448741, l1: 0.194893, l2: 0.232084, l3: 0.141482, l4: 0.172168, l5: 0.141532, l6: 0.131942

[epoch:   1/100000, batch:    38/  187, ite: 19] train loss: 2.522128, tar: 0.773505 
l0: 0.425371, l1: 0.241274, l2: 0.249000, l3: 0.165293, l4: 0.207186, l5: 0.756799, l6: 0.187549

[epoch:   1/100000, batch:    40/  187, ite: 20] train loss: 2.507645, tar: 0.756098 
l0: 0.376743, l1: 0.342398, l2: 0.262177, l3: 0.212977, l4: 0.226328, l5: 0.289582, l6: 0.195257

[epoch:   1/100000, batch:    42/  187, ite: 21] train loss: 2.478969, tar: 0.738034 
l0: 0.362785, l1: 0.150113, l2: 0.220138, l3: 0.131414, l4: 0.178391, l5: 0.733611, l6: 0.128347

[epoch:   1/100000, batch:    44/  187, ite: 22] train loss: 2.452871, tar: 0.720977 
l0: 0.295548, l1: 0.184224, l2: 0.220157, l3: 0.146341, l4: 0.209529, l5: 0.117924, l6: 0.153825

[epoch:   1/100000, batch:    46/  187, ite: 23] train loss: 2.403944, tar: 0.702480 
l0: 0.282655, l1: 0.082355, l2: 0.169598, l3: 0.101695, l4: 0.179073, l5: 0.273099, l6: 0.128187

[epoch:   1/100000, batch:    48/  187, ite: 24] train loss: 2.354474, tar: 0.684987 
l0: 0.352163, l1: 0.265759, l2: 0.243970, l3: 0.189555, l4: 0.253714, l5: 1.568810, l6: 0.236185

[epoch:   1/100000, batch:    50/  187, ite: 25] train loss: 2.384701, tar: 0.671674 
l0: 0.263430, l1: 0.100046, l2: 0.164968, l3: 0.113093, l4: 0.192880, l5: 0.224721, l6: 0.171697

[epoch:   1/100000, batch:    52/  187, ite: 26] train loss: 2.340321, tar: 0.655973 
l0: 0.299410, l1: 0.227567, l2: 0.185996, l3: 0.141164, l4: 0.192185, l5: 0.440046, l6: 0.189916

[epoch:   1/100000, batch:    54/  187, ite: 27] train loss: 2.315727, tar: 0.642766 
l0: 0.340161, l1: 0.298868, l2: 0.220347, l3: 0.195644, l4: 0.224114, l5: 1.260600, l6: 0.229478

[epoch:   1/100000, batch:    56/  187, ite: 28] train loss: 2.331923, tar: 0.631959 
l0: 0.262484, l1: 0.060241, l2: 0.115508, l3: 0.080237, l4: 0.114910, l5: 0.039081, l6: 0.072624

[epoch:   1/100000, batch:    58/  187, ite: 29] train loss: 2.277205, tar: 0.619219 
l0: 0.286898, l1: 0.204053, l2: 0.168288, l3: 0.151931, l4: 0.169166, l5: 0.231730, l6: 0.136679

[epoch:   1/100000, batch:    60/  187, ite: 30] train loss: 2.246256, tar: 0.608141 
l0: 0.289929, l1: 0.191591, l2: 0.158984, l3: 0.130925, l4: 0.156745, l5: 0.130693, l6: 0.128979

[epoch:   1/100000, batch:    62/  187, ite: 31] train loss: 2.212114, tar: 0.597876 
l0: 0.307384, l1: 0.282421, l2: 0.185637, l3: 0.168482, l4: 0.170624, l5: 0.248857, l6: 0.185017

[epoch:   1/100000, batch:    64/  187, ite: 32] train loss: 2.191373, tar: 0.588798 
l0: 0.305519, l1: 0.393390, l2: 0.215062, l3: 0.202035, l4: 0.195938, l5: 0.223193, l6: 0.205038

[epoch:   1/100000, batch:    66/  187, ite: 33] train loss: 2.177701, tar: 0.580214 
l0: 0.281309, l1: 0.198207, l2: 0.182001, l3: 0.156141, l4: 0.169080, l5: 0.472916, l6: 0.173946

[epoch:   1/100000, batch:    68/  187, ite: 34] train loss: 2.161698, tar: 0.571423 
l0: 0.273348, l1: 0.182190, l2: 0.149559, l3: 0.088483, l4: 0.110181, l5: 0.082138, l6: 0.086475

[epoch:   1/100000, batch:    70/  187, ite: 35] train loss: 2.127717, tar: 0.562906 
l0: 0.229414, l1: 0.172505, l2: 0.167869, l3: 0.112918, l4: 0.153939, l5: 0.114208, l6: 0.112918

[epoch:   1/100000, batch:    72/  187, ite: 36] train loss: 2.098163, tar: 0.553643 
l0: 0.232232, l1: 0.277148, l2: 0.191801, l3: 0.152058, l4: 0.186215, l5: 0.177533, l6: 0.165403

[epoch:   1/100000, batch:    74/  187, ite: 37] train loss: 2.078818, tar: 0.544956 
l0: 0.273337, l1: 0.455011, l2: 0.213751, l3: 0.225523, l4: 0.230144, l5: 0.667077, l6: 0.236735

[epoch:   1/100000, batch:    76/  187, ite: 38] train loss: 2.084680, tar: 0.537808 
l0: 0.206221, l1: 0.099445, l2: 0.139046, l3: 0.104106, l4: 0.137145, l5: 0.085523, l6: 0.093472

[epoch:   1/100000, batch:    78/  187, ite: 39] train loss: 2.053405, tar: 0.529306 
l0: 0.254785, l1: 0.261944, l2: 0.173801, l3: 0.161737, l4: 0.171249, l5: 0.218009, l6: 0.174086

[epoch:   1/100000, batch:    80/  187, ite: 40] train loss: 2.037460, tar: 0.522443 
l0: 0.227607, l1: 0.102913, l2: 0.130690, l3: 0.099392, l4: 0.124771, l5: 0.099322, l6: 0.107397

[epoch:   1/100000, batch:    82/  187, ite: 41] train loss: 2.009524, tar: 0.515252 
l0: 0.257783, l1: 0.416713, l2: 0.204032, l3: 0.195706, l4: 0.195412, l5: 0.291332, l6: 0.189166

[epoch:   1/100000, batch:    84/  187, ite: 42] train loss: 2.003349, tar: 0.509122 
l0: 0.253300, l1: 0.291380, l2: 0.174228, l3: 0.145187, l4: 0.169263, l5: 0.172793, l6: 0.148882

[epoch:   1/100000, batch:    86/  187, ite: 43] train loss: 1.988272, tar: 0.503172 
l0: 0.216201, l1: 0.210294, l2: 0.162498, l3: 0.143357, l4: 0.164202, l5: 0.225012, l6: 0.136875

[epoch:   1/100000, batch:    88/  187, ite: 44] train loss: 1.971684, tar: 0.496650 
l0: 0.206244, l1: 0.215323, l2: 0.150573, l3: 0.133192, l4: 0.150058, l5: 0.208513, l6: 0.137894

[epoch:   1/100000, batch:    90/  187, ite: 45] train loss: 1.954576, tar: 0.490197 
l0: 0.213753, l1: 0.201005, l2: 0.151887, l3: 0.129301, l4: 0.152162, l5: 0.153699, l6: 0.128599

[epoch:   1/100000, batch:    92/  187, ite: 46] train loss: 1.936659, tar: 0.484187 
l0: 0.214542, l1: 0.124815, l2: 0.138023, l3: 0.108320, l4: 0.114500, l5: 0.106906, l6: 0.106527

[epoch:   1/100000, batch:    94/  187, ite: 47] train loss: 1.914893, tar: 0.478450 
l0: 0.200015, l1: 0.212389, l2: 0.150199, l3: 0.121956, l4: 0.137952, l5: 0.102850, l6: 0.107314

[epoch:   1/100000, batch:    96/  187, ite: 48] train loss: 1.896513, tar: 0.472649 
l0: 0.168642, l1: 0.122887, l2: 0.135394, l3: 0.113731, l4: 0.127046, l5: 0.126540, l6: 0.118901

[epoch:   1/100000, batch:    98/  187, ite: 49] train loss: 1.876444, tar: 0.466445 
l0: 0.212846, l1: 0.296075, l2: 0.172290, l3: 0.155220, l4: 0.170314, l5: 0.275748, l6: 0.168552

[epoch:   1/100000, batch:   100/  187, ite: 50] train loss: 1.867936, tar: 0.461373 
l0: 0.182816, l1: 0.135355, l2: 0.132162, l3: 0.113632, l4: 0.120948, l5: 0.123444, l6: 0.110946

[epoch:   1/100000, batch:   102/  187, ite: 51] train loss: 1.849336, tar: 0.455911 
l0: 0.216392, l1: 0.295694, l2: 0.171109, l3: 0.158509, l4: 0.165640, l5: 0.168532, l6: 0.162094

[epoch:   1/100000, batch:   104/  187, ite: 52] train loss: 1.839502, tar: 0.451305 
l0: 0.180013, l1: 0.106779, l2: 0.125944, l3: 0.111586, l4: 0.124045, l5: 0.106206, l6: 0.107184

[epoch:   1/100000, batch:   106/  187, ite: 53] train loss: 1.821054, tar: 0.446186 
l0: 0.234337, l1: 0.483758, l2: 0.213724, l3: 0.220237, l4: 0.254937, l5: 0.305014, l6: 0.244768

[epoch:   1/100000, batch:   108/  187, ite: 54] train loss: 1.823567, tar: 0.442263 
l0: 0.239338, l1: 0.346355, l2: 0.187027, l3: 0.180637, l4: 0.196387, l5: 0.246590, l6: 0.182042

[epoch:   1/100000, batch:   110/  187, ite: 55] train loss: 1.819109, tar: 0.438574 
l0: 0.220077, l1: 0.149332, l2: 0.142209, l3: 0.121101, l4: 0.146142, l5: 0.122454, l6: 0.117310

[epoch:   1/100000, batch:   112/  187, ite: 56] train loss: 1.804815, tar: 0.434672 
l0: 0.244838, l1: 0.126765, l2: 0.141598, l3: 0.120729, l4: 0.122392, l5: 0.141254, l6: 0.102861

[epoch:   1/100000, batch:   114/  187, ite: 57] train loss: 1.790703, tar: 0.431341 
l0: 0.209056, l1: 0.167031, l2: 0.150858, l3: 0.158314, l4: 0.151730, l5: 0.151168, l6: 0.143803

[epoch:   1/100000, batch:   116/  187, ite: 58] train loss: 1.779345, tar: 0.427509 
l0: 0.203996, l1: 0.131280, l2: 0.135315, l3: 0.110225, l4: 0.136150, l5: 0.105105, l6: 0.110363

[epoch:   1/100000, batch:   118/  187, ite: 59] train loss: 1.764991, tar: 0.423721 
l0: 0.189097, l1: 0.081477, l2: 0.113306, l3: 0.079191, l4: 0.110878, l5: 0.087711, l6: 0.076322

[epoch:   1/100000, batch:   120/  187, ite: 60] train loss: 1.747874, tar: 0.419810 
l0: 0.205156, l1: 0.141887, l2: 0.150645, l3: 0.135118, l4: 0.143729, l5: 0.126949, l6: 0.120976

[epoch:   1/100000, batch:   122/  187, ite: 61] train loss: 1.736015, tar: 0.416291 
l0: 0.239541, l1: 0.424820, l2: 0.214528, l3: 0.204145, l4: 0.169001, l5: 0.199431, l6: 0.175840

[epoch:   1/100000, batch:   124/  187, ite: 62] train loss: 1.734261, tar: 0.413440 
l0: 0.209663, l1: 0.195669, l2: 0.162446, l3: 0.140994, l4: 0.158470, l5: 0.138794, l6: 0.130675

[epoch:   1/100000, batch:   126/  187, ite: 63] train loss: 1.724776, tar: 0.410206 
l0: 0.232452, l1: 0.192204, l2: 0.141874, l3: 0.119057, l4: 0.129887, l5: 0.112697, l6: 0.146549

[epoch:   1/100000, batch:   128/  187, ite: 64] train loss: 1.714619, tar: 0.407428 
l0: 0.250900, l1: 0.397064, l2: 0.201995, l3: 0.199953, l4: 0.215079, l5: 0.478103, l6: 0.268727

[epoch:   1/100000, batch:   130/  187, ite: 65] train loss: 1.719192, tar: 0.405020 
l0: 0.184411, l1: 0.176723, l2: 0.132015, l3: 0.122080, l4: 0.130845, l5: 0.107163, l6: 0.143550

[epoch:   1/100000, batch:   132/  187, ite: 66] train loss: 1.708246, tar: 0.401678 
l0: 0.214553, l1: 0.246568, l2: 0.160273, l3: 0.152492, l4: 0.163493, l5: 0.162263, l6: 0.185181

[epoch:   1/100000, batch:   134/  187, ite: 67] train loss: 1.701926, tar: 0.398885 
l0: 0.313661, l1: 0.502660, l2: 0.246180, l3: 0.256865, l4: 0.267625, l5: 1.002512, l6: 0.272919

[epoch:   1/100000, batch:   136/  187, ite: 68] train loss: 1.718992, tar: 0.397632 
l0: 0.198552, l1: 0.220580, l2: 0.133869, l3: 0.139064, l4: 0.134179, l5: 0.144833, l6: 0.152532

[epoch:   1/100000, batch:   138/  187, ite: 69] train loss: 1.710364, tar: 0.394746 
l0: 0.194841, l1: 0.179350, l2: 0.143160, l3: 0.130148, l4: 0.139337, l5: 0.135363, l6: 0.141403

[epoch:   1/100000, batch:   140/  187, ite: 70] train loss: 1.701124, tar: 0.391891 
l0: 0.217100, l1: 0.238100, l2: 0.157835, l3: 0.155835, l4: 0.159553, l5: 0.145035, l6: 0.161054

[epoch:   1/100000, batch:   142/  187, ite: 71] train loss: 1.694552, tar: 0.389429 
l0: 0.190678, l1: 0.153838, l2: 0.135105, l3: 0.114863, l4: 0.129312, l5: 0.112648, l6: 0.121575

[epoch:   1/100000, batch:   144/  187, ite: 72] train loss: 1.684323, tar: 0.386668 
l0: 0.184526, l1: 0.158155, l2: 0.141700, l3: 0.113164, l4: 0.131253, l5: 0.118063, l6: 0.119019

[epoch:   1/100000, batch:   146/  187, ite: 73] train loss: 1.674481, tar: 0.383899 
l0: 0.216304, l1: 0.272561, l2: 0.183123, l3: 0.155400, l4: 0.171405, l5: 0.198503, l6: 0.153807

[epoch:   1/100000, batch:   148/  187, ite: 74] train loss: 1.670111, tar: 0.381634 
l0: 0.214957, l1: 0.291211, l2: 0.186446, l3: 0.184144, l4: 0.182390, l5: 0.192294, l6: 0.252818

[epoch:   1/100000, batch:   150/  187, ite: 75] train loss: 1.667900, tar: 0.379412 
l0: 0.220430, l1: 0.248631, l2: 0.182226, l3: 0.168669, l4: 0.162716, l5: 0.165498, l6: 0.218256

[epoch:   1/100000, batch:   152/  187, ite: 76] train loss: 1.663933, tar: 0.377320 
l0: 0.175328, l1: 0.169511, l2: 0.144887, l3: 0.131310, l4: 0.142295, l5: 0.116278, l6: 0.193015

[epoch:   1/100000, batch:   154/  187, ite: 77] train loss: 1.656253, tar: 0.374697 
l0: 0.186905, l1: 0.230514, l2: 0.168899, l3: 0.158281, l4: 0.170821, l5: 0.181525, l6: 0.180803

[epoch:   1/100000, batch:   156/  187, ite: 78] train loss: 1.651401, tar: 0.372289 
l0: 0.208279, l1: 0.165447, l2: 0.148716, l3: 0.136081, l4: 0.138939, l5: 0.151706, l6: 0.190854

[epoch:   1/100000, batch:   158/  187, ite: 79] train loss: 1.644928, tar: 0.370213 
l0: 0.220712, l1: 0.253836, l2: 0.159567, l3: 0.158518, l4: 0.162687, l5: 0.152236, l6: 0.156072

[epoch:   1/100000, batch:   160/  187, ite: 80] train loss: 1.640161, tar: 0.368344 
l0: 0.180019, l1: 0.080028, l2: 0.098580, l3: 0.081885, l4: 0.083311, l5: 0.059215, l6: 0.072544

[epoch:   1/100000, batch:   162/  187, ite: 81] train loss: 1.628006, tar: 0.366019 
l0: 0.310673, l1: 0.578736, l2: 0.253271, l3: 0.272461, l4: 0.230703, l5: 0.293666, l6: 0.227233

[epoch:   1/100000, batch:   164/  187, ite: 82] train loss: 1.634576, tar: 0.365345 
l0: 0.219657, l1: 0.160787, l2: 0.146746, l3: 0.124484, l4: 0.131836, l5: 0.163625, l6: 0.149199

[epoch:   1/100000, batch:   166/  187, ite: 83] train loss: 1.628091, tar: 0.363589 
l0: 0.226822, l1: 0.211606, l2: 0.171374, l3: 0.163252, l4: 0.168970, l5: 0.162444, l6: 0.194745

[epoch:   1/100000, batch:   168/  187, ite: 84] train loss: 1.624176, tar: 0.361961 
l0: 0.221927, l1: 0.197179, l2: 0.165007, l3: 0.145655, l4: 0.160758, l5: 0.167454, l6: 0.219213

[epoch:   1/100000, batch:   170/  187, ite: 85] train loss: 1.620094, tar: 0.360314 
l0: 0.167544, l1: 0.039613, l2: 0.100450, l3: 0.063073, l4: 0.111695, l5: 0.048573, l6: 0.049736

[epoch:   1/100000, batch:   172/  187, ite: 86] train loss: 1.608008, tar: 0.358072 
l0: 0.212435, l1: 0.230057, l2: 0.171911, l3: 0.170830, l4: 0.194340, l5: 0.262064, l6: 0.227458

[epoch:   1/100000, batch:   174/  187, ite: 87] train loss: 1.606411, tar: 0.356398 
l0: 0.209365, l1: 0.203267, l2: 0.164128, l3: 0.157302, l4: 0.167723, l5: 0.166700, l6: 0.198478

[epoch:   1/100000, batch:   176/  187, ite: 88] train loss: 1.602554, tar: 0.354727 
l0: 0.142620, l1: 0.053468, l2: 0.090384, l3: 0.066871, l4: 0.081014, l5: 0.060056, l6: 0.077342

[epoch:   1/100000, batch:   178/  187, ite: 89] train loss: 1.590972, tar: 0.352344 
l0: 0.203165, l1: 0.248227, l2: 0.147841, l3: 0.148842, l4: 0.146821, l5: 0.188725, l6: 0.237332

[epoch:   1/100000, batch:   180/  187, ite: 90] train loss: 1.587971, tar: 0.350687 
l0: 0.190052, l1: 0.146840, l2: 0.119878, l3: 0.104616, l4: 0.110617, l5: 0.097721, l6: 0.109525

[epoch:   1/100000, batch:   182/  187, ite: 91] train loss: 1.580183, tar: 0.348921 
l0: 0.187899, l1: 0.199195, l2: 0.141942, l3: 0.145310, l4: 0.143391, l5: 0.159425, l6: 0.170088

[epoch:   1/100000, batch:   184/  187, ite: 92] train loss: 1.575477, tar: 0.347171 
l0: 0.148967, l1: 0.173120, l2: 0.121295, l3: 0.099242, l4: 0.113051, l5: 0.124122, l6: 0.105332

[epoch:   1/100000, batch:   186/  187, ite: 93] train loss: 1.568054, tar: 0.345040 
l0: 0.195278, l1: 0.270830, l2: 0.127399, l3: 0.122083, l4: 0.120183, l5: 0.130426, l6: 0.111894

[epoch:   1/100000, batch:   188/  187, ite: 94] train loss: 1.562842, tar: 0.343447 
/home/supreme/.local/lib/python3.7/site-packages/torch/nn/functional.py:3328: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
/home/supreme/.local/lib/python3.7/site-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  "See the documentation of nn.Upsample for details.".format(mode)
/home/supreme/.local/lib/python3.7/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
l0: 0.180364, l1: 0.257867, l2: 0.148948, l3: 0.144945, l4: 0.148590, l5: 0.157506, l6: 0.127730

[epoch:   2/100000, batch:     2/  187, ite: 95] train loss: 1.558664, tar: 0.341730 
l0: 0.160325, l1: 0.193894, l2: 0.120214, l3: 0.117743, l4: 0.118900, l5: 0.102683, l6: 0.107564

[epoch:   2/100000, batch:     4/  187, ite: 96] train loss: 1.552025, tar: 0.339840 
l0: 0.183971, l1: 0.322278, l2: 0.169746, l3: 0.161955, l4: 0.162981, l5: 0.217819, l6: 0.170668

[epoch:   2/100000, batch:     6/  187, ite: 97] train loss: 1.550349, tar: 0.338233 
l0: 0.199188, l1: 0.241954, l2: 0.148071, l3: 0.143998, l4: 0.147677, l5: 0.153610, l6: 0.189954

[epoch:   2/100000, batch:     8/  187, ite: 98] train loss: 1.547023, tar: 0.336815 
l0: 0.170217, l1: 0.104848, l2: 0.106500, l3: 0.092977, l4: 0.112492, l5: 0.116403, l6: 0.120781

[epoch:   2/100000, batch:    10/  187, ite: 99] train loss: 1.539722, tar: 0.335132 
l0: 0.181815, l1: 0.120189, l2: 0.113503, l3: 0.111433, l4: 0.112328, l5: 0.095591, l6: 0.096102

[epoch:   2/100000, batch:    12/  187, ite: 100] train loss: 1.532635, tar: 0.333599 
l0: 0.322100, l1: 0.478439, l2: 0.259932, l3: 0.276068, l4: 0.278996, l5: 1.505428, l6: 0.308371

[epoch:   2/100000, batch:    14/  187, ite: 101] train loss: 1.551414, tar: 0.333485 
l0: 0.165867, l1: 0.210640, l2: 0.143775, l3: 0.131137, l4: 0.147846, l5: 0.149582, l6: 0.196601

[epoch:   2/100000, batch:    16/  187, ite: 102] train loss: 1.547434, tar: 0.331841 
l0: 0.228042, l1: 0.358819, l2: 0.186358, l3: 0.189510, l4: 0.174487, l5: 0.189145, l6: 0.209430

[epoch:   2/100000, batch:    18/  187, ite: 103] train loss: 1.547321, tar: 0.330834 
l0: 0.243232, l1: 0.298610, l2: 0.198773, l3: 0.203296, l4: 0.202601, l5: 0.763579, l6: 0.219383

[epoch:   2/100000, batch:    20/  187, ite: 104] train loss: 1.552918, tar: 0.329991 
l0: 0.180488, l1: 0.164481, l2: 0.113821, l3: 0.097886, l4: 0.117275, l5: 0.098498, l6: 0.113588

[epoch:   2/100000, batch:    22/  187, ite: 105] train loss: 1.546567, tar: 0.328568 
l0: 0.204631, l1: 0.301067, l2: 0.177358, l3: 0.196441, l4: 0.181057, l5: 0.189534, l6: 0.191039

[epoch:   2/100000, batch:    24/  187, ite: 106] train loss: 1.545573, tar: 0.327398 
l0: 0.189126, l1: 0.262095, l2: 0.173366, l3: 0.155396, l4: 0.172234, l5: 0.176346, l6: 0.173642

[epoch:   2/100000, batch:    26/  187, ite: 107] train loss: 1.543298, tar: 0.326106 
l0: 0.184370, l1: 0.201578, l2: 0.134199, l3: 0.134588, l4: 0.134344, l5: 0.127549, l6: 0.156814

[epoch:   2/100000, batch:    28/  187, ite: 108] train loss: 1.538948, tar: 0.324794 
l0: 0.187608, l1: 0.088196, l2: 0.121315, l3: 0.105147, l4: 0.130029, l5: 0.089307, l6: 0.123108

[epoch:   2/100000, batch:    30/  187, ite: 109] train loss: 1.532578, tar: 0.323535 
l0: 0.243325, l1: 0.257718, l2: 0.166832, l3: 0.171636, l4: 0.165574, l5: 0.182399, l6: 0.163242

[epoch:   2/100000, batch:    32/  187, ite: 110] train loss: 1.530925, tar: 0.322806 
l0: 0.227787, l1: 0.285575, l2: 0.183440, l3: 0.192605, l4: 0.182557, l5: 0.195072, l6: 0.182684

[epoch:   2/100000, batch:    34/  187, ite: 111] train loss: 1.530194, tar: 0.321950 
l0: 0.183473, l1: 0.187305, l2: 0.142847, l3: 0.142177, l4: 0.151603, l5: 0.142057, l6: 0.137686

[epoch:   2/100000, batch:    36/  187, ite: 112] train loss: 1.526238, tar: 0.320714 
l0: 0.176511, l1: 0.092822, l2: 0.109586, l3: 0.090474, l4: 0.104397, l5: 0.091187, l6: 0.091524

[epoch:   2/100000, batch:    38/  187, ite: 113] train loss: 1.519426, tar: 0.319437 
l0: 0.194745, l1: 0.208081, l2: 0.146688, l3: 0.145734, l4: 0.147365, l5: 0.146416, l6: 0.173022

[epoch:   2/100000, batch:    40/  187, ite: 114] train loss: 1.516291, tar: 0.318344 
l0: 0.196038, l1: 0.158181, l2: 0.128983, l3: 0.110452, l4: 0.128660, l5: 0.118631, l6: 0.133334

[epoch:   2/100000, batch:    42/  187, ite: 115] train loss: 1.511578, tar: 0.317280 
l0: 0.172099, l1: 0.128266, l2: 0.124016, l3: 0.105259, l4: 0.126471, l5: 0.116670, l6: 0.112862

[epoch:   2/100000, batch:    44/  187, ite: 116] train loss: 1.506182, tar: 0.316028 
l0: 0.184393, l1: 0.160285, l2: 0.134461, l3: 0.117362, l4: 0.130691, l5: 0.123518, l6: 0.159314

[epoch:   2/100000, batch:    46/  187, ite: 117] train loss: 1.501941, tar: 0.314903 
l0: 0.178731, l1: 0.202423, l2: 0.143654, l3: 0.128227, l4: 0.144199, l5: 0.136536, l6: 0.141621

[epoch:   2/100000, batch:    48/  187, ite: 118] train loss: 1.498327, tar: 0.313749 
l0: 0.192373, l1: 0.324630, l2: 0.188281, l3: 0.188924, l4: 0.185140, l5: 0.201505, l6: 0.194968

[epoch:   2/100000, batch:    50/  187, ite: 119] train loss: 1.498137, tar: 0.312729 
l0: 0.155332, l1: 0.146133, l2: 0.119225, l3: 0.103263, l4: 0.117182, l5: 0.107761, l6: 0.112619

[epoch:   2/100000, batch:    52/  187, ite: 120] train loss: 1.492832, tar: 0.311418 
l0: 0.202824, l1: 0.212355, l2: 0.134606, l3: 0.125806, l4: 0.137556, l5: 0.176907, l6: 0.137657

[epoch:   2/100000, batch:    54/  187, ite: 121] train loss: 1.489815, tar: 0.310520 
l0: 0.206580, l1: 0.277555, l2: 0.169986, l3: 0.179364, l4: 0.167326, l5: 0.219450, l6: 0.220343

[epoch:   2/100000, batch:    56/  187, ite: 122] train loss: 1.489411, tar: 0.309668 
l0: 0.175496, l1: 0.171662, l2: 0.132292, l3: 0.127838, l4: 0.129943, l5: 0.138356, l6: 0.153416

[epoch:   2/100000, batch:    58/  187, ite: 123] train loss: 1.485668, tar: 0.308578 
l0: 0.149539, l1: 0.101616, l2: 0.118334, l3: 0.111743, l4: 0.124950, l5: 0.099798, l6: 0.147533

[epoch:   2/100000, batch:    60/  187, ite: 124] train loss: 1.480570, tar: 0.307295 
l0: 0.133375, l1: 0.046298, l2: 0.087950, l3: 0.065393, l4: 0.085242, l5: 0.070615, l6: 0.106346

[epoch:   2/100000, batch:    62/  187, ite: 125] train loss: 1.473487, tar: 0.305904 
l0: 0.144338, l1: 0.099996, l2: 0.108031, l3: 0.097144, l4: 0.104852, l5: 0.103872, l6: 0.094091

[epoch:   2/100000, batch:    64/  187, ite: 126] train loss: 1.467764, tar: 0.304621 
l0: 0.208278, l1: 0.369624, l2: 0.163595, l3: 0.156973, l4: 0.158641, l5: 0.145740, l6: 0.175591

[epoch:   2/100000, batch:    66/  187, ite: 127] train loss: 1.467060, tar: 0.303863 
l0: 0.197969, l1: 0.287630, l2: 0.165385, l3: 0.163017, l4: 0.160277, l5: 0.154752, l6: 0.208283

[epoch:   2/100000, batch:    68/  187, ite: 128] train loss: 1.466047, tar: 0.303035 
l0: 0.141105, l1: 0.058491, l2: 0.089410, l3: 0.064056, l4: 0.068362, l5: 0.064726, l6: 0.063494

[epoch:   2/100000, batch:    70/  187, ite: 129] train loss: 1.458943, tar: 0.301780 
l0: 0.190021, l1: 0.219529, l2: 0.145345, l3: 0.133896, l4: 0.141619, l5: 0.134512, l6: 0.135413

[epoch:   2/100000, batch:    72/  187, ite: 130] train loss: 1.456184, tar: 0.300920 
l0: 0.190316, l1: 0.261629, l2: 0.150607, l3: 0.156683, l4: 0.157725, l5: 0.156721, l6: 0.172012

[epoch:   2/100000, batch:    74/  187, ite: 131] train loss: 1.454578, tar: 0.300076 
l0: 0.183568, l1: 0.219452, l2: 0.153559, l3: 0.135191, l4: 0.143402, l5: 0.148148, l6: 0.152536

[epoch:   2/100000, batch:    76/  187, ite: 132] train loss: 1.452163, tar: 0.299194 
l0: 0.167196, l1: 0.128991, l2: 0.128129, l3: 0.102586, l4: 0.124044, l5: 0.093583, l6: 0.097154

[epoch:   2/100000, batch:    78/  187, ite: 133] train loss: 1.447573, tar: 0.298201 
l0: 0.208420, l1: 0.300343, l2: 0.177552, l3: 0.170179, l4: 0.186778, l5: 0.228390, l6: 0.176688

[epoch:   2/100000, batch:    80/  187, ite: 134] train loss: 1.447579, tar: 0.297531 
l0: 0.193824, l1: 0.192631, l2: 0.152343, l3: 0.161594, l4: 0.150308, l5: 0.146540, l6: 0.143606

[epoch:   2/100000, batch:    82/  187, ite: 135] train loss: 1.445307, tar: 0.296763 
l0: 0.221775, l1: 0.320070, l2: 0.212874, l3: 0.221574, l4: 0.207471, l5: 0.218172, l6: 0.163009

[epoch:   2/100000, batch:    84/  187, ite: 136] train loss: 1.446186, tar: 0.296211 
l0: 0.175135, l1: 0.140468, l2: 0.140810, l3: 0.131101, l4: 0.148501, l5: 0.117103, l6: 0.133966

[epoch:   2/100000, batch:    86/  187, ite: 137] train loss: 1.442835, tar: 0.295328 
l0: 0.175859, l1: 0.150973, l2: 0.136592, l3: 0.121878, l4: 0.140766, l5: 0.104782, l6: 0.135706

[epoch:   2/100000, batch:    88/  187, ite: 138] train loss: 1.439384, tar: 0.294462 
l0: 0.192269, l1: 0.137508, l2: 0.141379, l3: 0.127562, l4: 0.142491, l5: 0.129960, l6: 0.154663
